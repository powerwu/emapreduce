# 准备工作

本文介绍Spark开发所需要的准备工作。

## 安装E-MapReduce SDK

您可以通过以下两种方法安装E-MapReduce SDK：

-   直接在Eclipse中使用JAR包，步骤如下：
    1.  从Maven库源下载E-MapReduce开发的[依赖包](http://mvnrepository.com/search?q=com.aliyun.emr)。
    2.  将所需的JAR包拷贝到您的工程文件夹中。（根据您运行作业集群的Spark版本选择SDK版本，Spark 1.x版本建议使用2.10，Spark 2.x建议使用2.11）
    3.  在Eclipse中工程的名字上单击右键，然后依次选择**Properties** \> **Java Build Path** \> **Add JARs**。
    4.  选择您下载的JAR包。

        完成后，您可以在工程中读写OSS、LogService、MNS、ONS、OTS和MaxCompute等数据。

-   创建Maven工程，添加如下依赖。

    ```
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-tablestore</artifactId>
            <version>1.9.0</version>
        </dependency>
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-mns_2.11</artifactId>
            <version>1.9.0</version>
        </dependency>
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-logservice_2.11</artifactId>
            <version>1.9.0</version>
        </dependency>
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-maxcompute_2.11</artifactId>
            <version>1.9.0</version>
        </dependency>
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-ons_2.11</artifactId>
            <version>1.9.0</version>
        </dependency>
        <dependency>
            <groupId>com.aliyun.emr</groupId>
            <artifactId>emr-datahub_2.11</artifactId>
            <version>1.9.0</version>
        </dependency>
    ```


## Spark代码本地调试

本地调试运行Spark代码读写OSS数据时，需要配置SparkConf，设置`spark.hadoop.mapreduce.job.run-local`为`true`，其余参数保持默认即可。示例代码如下。

-   EMR-3.24.0及后续版本

    ```
    val conf = new SparkConf().setAppName(getAppName).setMaster("local[4]")
       conf.set("spark.hadoop.fs.oss.impl", "com.aliyun.emr.fs.oss.JindoOssFileSystem")
       conf.set("spark.hadoop.mapreduce.job.run-local", "true")
       val sc = new SparkContext(conf) 
       val data = sc.textFile("oss://...")
       println(s"count: ${data.count()}")
    ```

-   EMR-3.24.0之前版本

    ```
    val conf = new SparkConf().setAppName(getAppName).setMaster("local[4]")
       conf.set("spark.hadoop.fs.oss.impl", "com.aliyun.fs.oss.nat.NativeOssFileSystem")
       conf.set("spark.hadoop.mapreduce.job.run-local", "true")
       val sc = new SparkContext(conf) 
       val data = sc.textFile("oss://...")
       println(s"count: ${data.count()}")
    ```


## 常见问题

-   三方依赖说明

    您的作业需要依赖三方包，即可在E-MapReduce上操作阿里云的数据源，例如OSS和MaxCompute等。

    增删需要依赖的三方包时，请参见[pom](https://github.com/aliyun/aliyun-emapreduce-demo/blob/master/pom.xml)。

-   OSS输出目录设置

    在本地的Hadoop配置文件中增加配置参数fs.oss.buffer.dirs，参数值是本地的目录路径。如果您没有设置这个参数值，则在本地运行Spark程序，写入OSS数据时提示空指针异常。

-   垃圾清理

    Spark作业失败后，产生的数据不会主动清理。因此当您的Spark作业运行失败后，请检查OSS输出目录是否有文件存在，并且检查OSS碎片管理中是否存在提交的碎片，如果存在请及时清理。

-   PySpark使用说明

    当您使用Python来编写Spark作业时，请参见[aliyun-emapreduce-sdk](https://github.com/aliyun/aliyun-emapreduce-sdk/blob/master/docs/how_to_run_spark_with_python_sdk.md)配置您的环境。


