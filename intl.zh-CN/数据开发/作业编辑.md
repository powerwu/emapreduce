# 作业编辑

在项目中，您可以通过创建作业来进行任务开发。目前E-MapReduce数据开发支持的作业类型有：Shell、Hive、Hive SQL、Spark、SparkSQL、Spark Shell、Spark Streaming、MR、Sqoop、Pig 、Flink、Streaming SQL、Presto SQL和Impala SQL。

已创建项目或已被加入到项目中，详情请参见[项目管理](/intl.zh-CN/数据开发/项目管理.md)。

## 新建作业

1.  已通过主账号登录[阿里云E-MapReduce控制台](https://emr.console.aliyun.com)。

2.  在顶部菜单栏处，根据实际情况选择地域（Region）和资源组。

3.  单击上方的**数据开发**页签.。

4.  单击待编辑项目所在行的**作业编辑**。

5.  在页面左侧，在需要操作的文件夹上单击右键，选择**新建作业**。

6.  在**新建作业**对话框中，输入**作业名称**和**作业描述**，从**作业类型**列表中，选择新建的作业类型。

    **说明：** 创建作业时**作业类型**一经确定，不能修改。

7.  单击**确定**。

    **说明：** 您还可以通过在文件夹上单击右键，进行**新建子文件夹**、**重命名文件夹**和**删除文件夹**操作。


## 设置作业

各个具体作业类型的开发与设置，请参见[作业](/intl.zh-CN/数据开发/作业/Shell作业配置.md)部分。 以下介绍的是作业的**基础设置**、**高级设置**和**告警设置**。

1.  单击页面右上角的**作业设置**。

2.  在**作业设置**页面，设置基础信息。

    |参数|描述|
    |--|--|
    |**作业概要**|    -   **失败重试次数**：设置在工作流运行到该作业失败时，最大支持重试的次数。

**说明：** 如果直接在作业页面运行作业，该选项不会生效。

    -   **失败策略**：设置在工作流运行到该作业失败时，是继续执行下一个工作流节点还是暂停当前工作流。
    -   **作业描述**：可修改作业的描述。 |
    |**运行资源**|单击右侧的![加号](https://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/zh-CN/0259929951/p66254.png)，添加作业执行所依赖的jar包或UDF等资源。您需要将资源先上传至OSS，然后在**运行资源**中直接添加即可。|
    |**配置参数**|指定作业代码中所引用的变量的值。您可以在代码中引用变量，格式为$\{变量名\}。 单击右侧的![加号](https://static-aliyun-doc.oss-cn-hangzhou.aliyuncs.com/assets/img/zh-CN/0259929951/p66254.png)，添加Key和Value。其中，Key为变量名，Value为变量的值。另外，您还可以根据调度启动时间在此配置时间变量，详情请参见[作业日期设置](/intl.zh-CN/数据开发/作业/作业日期设置.md)。 |

3.  在**作业设置**页面，单击**高级设置**。

    |配置项|说明|
    |---|--|
    |**模式**|    -   **提交节点**包括以下两种模式，详情请参见[作业编辑](/intl.zh-CN/数据开发/作业编辑.md)中的作业提交模式说明。
        -   **在 Worker 节点提交**：作业通过Launcher在YARN上分配资源进行提交。
        -   **在 Header/Gateway 节点提交**：作业在分配的机器上直接运行。
    -   **预期最大运行时长**：0~10800秒。 |
    |**环境变量**|添加作业执行的环境变量，也可以在作业脚本中直接export环境变量。     -   示例一：一个shell类型的任务，内容是`echo ${ENV_ABC}`。如果此处设置了一个环境变量 `ENV_ABC=12345`，则`echo`命令的输出结果为 `12345`。
    -   示例二：一个shell类型的作业，内容是`java -jar abc.jar`，其中abc.jar的内容是：

        ```
public static void main(String[] args) {System.out.println(System.getEnv("ENV_ABC"));}
        ```

您得到结果是`12345`。此处环境变量的设置相当于执行了以下脚本：

        ```
export ENV_ABC=12345
java -jar abc.jar
        ``` |
    |**调度参数**|设置作业运行YARN 队列、内存、虚拟核数、优先级和执行用户等信息。当未设置这些参数时，作业会直接采用Hadoop集群的默认值。 **说明：** 内存设置用于设置启动器Launcher的内存配额。 |

4.  高级设置完成后，单击**告警设置**，设置作业的告警配置项。

    |配置项|说明|
    |---|--|
    |**执行失败**|设置作业执行失败时，是否通知到用户告警组或钉钉告警组。|
    |**启动超时**|设置作业启动失败时，是否通知到用户告警组或钉钉告警组。|
    |**作业执行超时**|设置作业执行超时时，是否通知到用户告警组或钉钉告警组。|


## 执行作业

在新建的作业页面，单击右上方的**运行**来执行作业。

## 查看日志

作业运行后，您可以在页面下方的**运行记录**中，查看作业实例的运行日志。

单击运行记录右侧的**详情**，即可跳转至**运维中心**查看作业实例的详细信息。

## 作业可执行操作

在**作业编辑**区域，您可以在作业名称上单击右键，执行如下操作：

-   克隆作业：在相同文件夹下，克隆一个已经存在作业的配置。
-   重命名作业：重新命名作业名称。
-   删除作业：只有在作业没有关联工作流，或关联的工作流没有在运行或调度时中，才可以被删除。

## 作业提交模式说明

Spark-Submit进程（在数据开发模块中为启动器Launcher）是Spark的作业提交命令，用于提交Spark作业，一般占用600 MB以上内存。作业配置面板中的内存设置，用于设置Launcher的内存配额。

新版作业提交模式包括以下两种：

-   **在Header/Gateway节点提交**：Spark-Submit进程运行在Header节点上，不受YARN监控。Spark-Submit内存消耗大，作业过多会造成Header节点资源紧张，导致整个集群不稳定。
-   **在Worker节点提交**： Spark-Submit进程运行在Worker节点上，占用YARN的一个Container，受YARN监控。此模式可以缓解Header节点的资源使用。

在E-MapReduce集群中，作业实例消耗内存=Launcher消耗内存+用户作业（Job）消耗内存。对于Spark作业，Job消耗内存又可以进一步细分，即Job消耗内存 = spark-submit（指逻辑模块，非进程）消耗内存+driver端消耗内存+executor端消耗内存。作业配置不同，Driver端消耗的物理内存的位置也不同，详细说明如下：

-   如果Spark使用Yarn-Client模式，则Spark-Submit+Driver是在同一个进程中。在作业提交中，如果这个进程使用LOCAL模式，则这个进程是 Header节点上的一个进程，不受YARN监控。如果这个进程使用YARN模式，则这个进程是Worker节点上的一个进程，占YARN的一个 Container，受YARN监控。
-   如果Spark使用Yarn-Cluster模式，则Driver是独立的一个进程，占用YARN的一个Container，与Spark-Submit不在一个进程中。

综上所述，作业的提交节点决定了Spark-Submit进程是在Header节点还是Worker节点上运行，以及是否受YARN监控。Spark的Yarn-Client和Yarn-Cluster模式，决定了Driver是否与Spark-Submit在同一个进程中。

## 添加注解

进行数据开发时，您可以通过在作业内容里添加特定的注解来添加作业参数。注解的格式如下。

```
!!! @<注解名称>: <注解内容>
```

**说明：** `!!!`必须顶格，并且每行一个注解。

当前支持的注解如下。

|注解名称|说明|示例|
|----|--|--|
|rem|表示一行注释。|```
!!! @rem: 这是一行注释
``` |
|env|添加一个环境变量。|```
!!! @env: ENV_1=ABC
``` |
|var|添加一个自定义变量。|```
!!! @var: var1="value1 and \"one string end with 3 spaces\"   "
!!! @var: var2=${yyyy-MM-dd}
``` |
|resource|添加一个资源文件。|```
!!! @resource: oss://bucket1/dir1/file.jar
``` |
|sharedlibs|添加依赖库，仅对Streaming SQL作业有效。包含多个依赖库时，依赖库间用英文半角逗号（,）隔开。|```
!!! @sharedlibs: sharedlibs:streamingsql:datasources-bundle:1.7.0,...
``` |
|scheduler.queue|设置提交队列。|```
!!! @scheduler.queue: default
``` |
|scheduler.vmem|设置申请内存，单位MB。|```
!!! @scheduler.vmem: 1024
``` |
|scheduler.vcores|设置申请的核数。|```
!!! @scheduler.vcores: 1
``` |
|scheduler.priority|设置申请的优先级，取值范围为1~100。|```
!!! @scheduler.priority: 1
``` |
|scheduler.user|设置提交用户名。|```
!!! @scheduler.user: root
``` |

**说明：**

使用注解时，需要注意以下事项：

-   无效注解将被自动跳过。例如，设置未知注解、注解内容不符合预期等。
-   注解中的作业参数优先级高于作业配置中的参数，如果作业注解和作业配置中有相同的参数，则以作业注解为准。

